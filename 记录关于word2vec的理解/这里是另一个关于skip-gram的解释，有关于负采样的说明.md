Skip-gram 模型是 Word2Vec 中的一种架构，它被设计用于学习词嵌入（word embeddings），即将词汇映射到连续的向量空间中。Skip-gram 模型的目标是从一个词预测其周围的上下文词，而不是像 CBOW（Continuous Bag of Words）那样从上下文词预测目标词。

### Skip-gram 模型训练数据构造

构建 Skip-gram 模型的训练数据涉及以下几个步骤：

1. **文本预处理**:
   - 清洗文本数据，去除标点符号、数字、停用词等。
   - 分词，将文本分解成单词序列。

2. **创建词汇表**:
   - 统计文本中所有词的频率，构建一个词汇表。
   - 为词汇表中的每个词分配一个唯一的索引。

3. **生成词对（Target-Context Pairs）**:
   - 从清洗后的文本中滑动一个窗口，窗口大小为 `2 * window_size + 1`，其中 `window_size` 是预先设定的参数，表示上下文词距离目标词的最大位置偏移量。
   - 对于窗口内的每个词，将其视为“目标词”，并将窗口内的其他词视为“上下文词”。
   - 生成一系列 `(target_word, context_word)` 对。

例如，假设我们的句子是 "the quick brown fox jumps over the lazy dog"，窗口大小为 2，则生成的词对可能如下：

- ("quick", "the")
- ("quick", "brown")
- ("brown", "quick")
- ("brown", "fox")
- ...
- ("lazy", "jumps")
- ("lazy", "dog")

4. **正样本与负样本**:
   - 正样本是实际出现在文本中的 `(target_word, context_word)` 对。
   - 负样本是随机生成的 `(target_word, false_context_word)` 对，其中 `false_context_word` 不是实际与 `target_word` 相关联的上下文词。生成负样本是为了帮助模型学习区分真实的上下文词和非上下文词。

### Skip-gram 模型训练

Skip-gram 模型通过最大化目标词预测周围上下文词的概率来学习词嵌入。在训练过程中，模型试图优化词嵌入矩阵，使得对于给定的目标词，其周围的上下文词具有较高的概率，而非上下文词具有较低的概率。

在每次训练迭代中，模型都会更新词嵌入矩阵，以提高对正样本的预测准确性和降低对负样本的预测准确性。通过多次迭代，词嵌入矩阵逐渐收敛，形成对词汇语义和语法结构的有效表示。

### 总结

Skip-gram 模型的训练数据构造主要是围绕创建目标词与其上下文词的配对，这涉及到滑动窗口技术以及可能的负采样策略。通过这种构造方式，模型能够在大规模语料库上学习到高质量的词嵌入。